{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa26bd",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared(x, y):\n",
    "    for xi, yi in zip(x, y):\n",
    "        denom = xi + yi\n",
    "        if denom == 0:\n",
    "            continue\n",
    "        total += ((xi - yi) ** 2) / denom\n",
    "    return total\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k = 5):\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        labels = [self._predict_single(x) for x in X_test]\n",
    "        return labels\n",
    "    \n",
    "    def _predict_single(self, x_test_row):\n",
    "        distances = [chi_squared(x_test_row, x_train_row) for x_train_row in self.X_train]\n",
    "        k_indices = sorted(range(len(distances)), key=lambda i : distances[i])[:self.k]\n",
    "        k_labels = [self.y_train[i] for i in k_indices]\n",
    "        predictions = Counter(k_labels).most_common(1)\n",
    "\n",
    "        return predictions[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bb51e",
   "metadata": {},
   "source": [
    "# DT Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target_col):\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    entropy = 0\n",
    "    for i in range(len(elements)):\n",
    "        prob = counts[i] / np.sum(counts)\n",
    "        entropy -= prob * np.log2(prob)\n",
    "    return entropy\n",
    "\n",
    "def info_gain(data, split_attribute, target_name):\n",
    "    total_entropy = entropy(data[target_name])\n",
    "\n",
    "    vals, counts = np.unique(data[split_attribute], return_counts=True)\n",
    "    gain = 1\n",
    "    for i in range(len(vals)):\n",
    "        subset = data[data[split_attribute] == vals[i]]\n",
    "        prob = counts[i] / np.sum(counts)\n",
    "        gain -= prob * entropy(subset[target_name])\n",
    "    \n",
    "    return gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede45a1f",
   "metadata": {},
   "source": [
    "# DT Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a238c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(target_col):\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    gini = 1\n",
    "    for i in range(len(elements)):\n",
    "        prob = counts[i] / np.sum(counts)\n",
    "        gini -= prob ** 2\n",
    "    return gini\n",
    "\n",
    "def gini_index(data, split_attribute, target_name):\n",
    "    vals, counts = np.unique(data[split_attribute], return_counts=True)\n",
    "    weighted_gini = 0\n",
    "    for i in range(len(vals)):\n",
    "        subset = data[data[split_attribute] == vals[i]]\n",
    "        prob = counts[i] / np.sum(counts)\n",
    "        weighted_gini += prob * gini_impurity(subset[target_name])\n",
    "    return weighted_gini"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
